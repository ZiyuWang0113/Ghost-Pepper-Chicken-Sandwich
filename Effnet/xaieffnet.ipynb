{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSEP6OGUSy7F"
      },
      "outputs": [],
      "source": [
        "!pip install timm==0.9.12 torch torchvision --quiet\n",
        "!pip install captum lime scikit-image matplotlib --quiet\n",
        "!pip install --quiet scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rMp3FmwTDYW"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from tqdm import tqdm\n",
        "import torch, torch.nn.functional as F\n",
        "import timm\n",
        "from timm.data import create_transform\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from skimage.segmentation import slic, mark_boundaries\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLezIy9LTFKz",
        "outputId": "f675e49d-813f-44f7-bf4a-1c793115d0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOJnZo6DTFjf"
      },
      "outputs": [],
      "source": [
        "DEVICE   = 'cuda' if torch.cuda.is_available() else 'cpu' # assuming use gpu on colab\n",
        "IMG_SIZE = 300\n",
        "TRANSFORM = create_transform(\n",
        "    IMG_SIZE,\n",
        "    is_training=False,\n",
        "    interpolation='bicubic',\n",
        "    mean=(0.485, 0.456, 0.406),\n",
        "    std =(0.229, 0.224, 0.225)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhBRSiHzTHTR"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT = '/content/drive/MyDrive/1430project/best_model.pth'\n",
        "MODEL_NAME = 'efficientnet_b3'\n",
        "model = timm.create_model(MODEL_NAME, pretrained=False, num_classes=2).to(DEVICE)\n",
        "model.load_state_dict(torch.load(CHECKPOINT, map_location=DEVICE))\n",
        "model.eval()\n",
        "class_names = ['fake', 'real']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB0jiKHgTIOo"
      },
      "outputs": [],
      "source": [
        "# Ty to Allison for standardizing this explainer in advance so it was easy to choose all params and all that\n",
        "class LimeExplainer:\n",
        "    def __init__(self, model, transform, device,\n",
        "                 num_samples=1000, num_superpixels=50,\n",
        "                 compactness=10, sigma=1, output_dir=None):\n",
        "        self.model           = model\n",
        "        self.transform       = transform\n",
        "        self.device          = device\n",
        "        self.num_samples     = num_samples\n",
        "        self.num_superpixels = num_superpixels\n",
        "        self.compactness     = compactness\n",
        "        self.sigma           = sigma\n",
        "        self.output_dir      = output_dir\n",
        "\n",
        "  # set up all core params\n",
        "\n",
        "    def segment_image(self, image):\n",
        "      # use SLIC to split into superpixesl\n",
        "        img_array = np.array(image)\n",
        "        segments  = slic(\n",
        "            img_array,\n",
        "            n_segments=self.num_superpixels,\n",
        "            compactness=self.compactness,\n",
        "            sigma=self.sigma,\n",
        "            start_label=1\n",
        "        )\n",
        "        return segments\n",
        "\n",
        "    def perturb_image(self, image, segments, perturb_mask):\n",
        "      # replace \"off\" segs w gray so we can test effect\n",
        "        img_array = np.array(image).copy()\n",
        "        gray = np.mean(img_array, axis=2, keepdims=True).repeat(3, axis=2)\n",
        "        for seg_id in range(1, np.max(segments)+1):\n",
        "            if not perturb_mask[seg_id-1]:\n",
        "                img_array[segments == seg_id] = gray[segments == seg_id]\n",
        "\n",
        "        return Image.fromarray(img_array.astype(np.uint8))\n",
        "\n",
        "    def get_model_prediction(self, image):\n",
        "        tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(tensor)\n",
        "            probs  = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        return probs\n",
        "\n",
        "    def explain(self, image, target_class=None):\n",
        "\n",
        "        initial_probs = self.get_model_prediction(image)\n",
        "        pred_cls      = int(np.argmax(initial_probs))\n",
        "\n",
        "        if target_class is None:\n",
        "            target_class = pred_cls\n",
        "\n",
        "\n",
        "        segments     = self.segment_image(image)\n",
        "        num_segments = int(np.max(segments))\n",
        "\n",
        "        perturbed_data, predictions = [], []\n",
        "        for i in tqdm(range(self.num_samples),\n",
        "                      desc=f\"Sampling for class {target_class}\"):\n",
        "        # randomly turn segments on/off and collect how it effects prediction\n",
        "\n",
        "            mask = np.random.randint(0, 2, num_segments, dtype=bool)\n",
        "            pert_img = self.perturb_image(image, segments, mask)\n",
        "            prob     = self.get_model_prediction(pert_img)[target_class]\n",
        "\n",
        "            perturbed_data.append(mask)\n",
        "            predictions.append(prob)\n",
        "\n",
        "        perturbed_data = np.array(perturbed_data)\n",
        "        predictions    = np.array(predictions)\n",
        "\n",
        "        explainer = Ridge(alpha=1.0)\n",
        "        explainer.fit(perturbed_data, predictions)\n",
        "        feat_imp = explainer.coef_\n",
        "\n",
        "        seg_imp = np.zeros(image.size[::-1], dtype=np.float32)\n",
        "        for seg_id in range(1, num_segments+1):\n",
        "            seg_imp[segments == seg_id] = feat_imp[seg_id-1]\n",
        "\n",
        "        r2 = r2_score(predictions, explainer.predict(perturbed_data))\n",
        "\n",
        "        return segments, seg_imp, feat_imp, r2, pred_cls, initial_probs\n",
        "\n",
        "\n",
        "    def visualize_explanation(self, image, segments, seg_imp,\n",
        "                              class_label, probability, r2,\n",
        "                              save_path=None):\n",
        "\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        if np.max(np.abs(seg_imp)) > 0:\n",
        "            seg_imp = seg_imp / np.max(np.abs(seg_imp))\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "        gs  = gridspec.GridSpec(2, 3, height_ratios=[1, 0.05])\n",
        "\n",
        "        # original\n",
        "        ax = plt.subplot(gs[0, 0])\n",
        "        ax.imshow(img_array)\n",
        "        ax.set_title('Original Image', fontsize=14)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # segmentation\n",
        "        ax = plt.subplot(gs[0, 1])\n",
        "        ax.imshow(mark_boundaries(img_array, segments))\n",
        "        ax.set_title(f'Segmentation ({np.max(segments)} superpixels)',\n",
        "                     fontsize=14)\n",
        "        ax.axis('off')\n",
        "        # heat-map\n",
        "        ax = plt.subplot(gs[0, 2])\n",
        "        cmap     = plt.cm.RdYlGn\n",
        "        heat_img = ax.imshow(seg_imp, cmap=cmap, vmin=-1, vmax=1)\n",
        "        ax.set_title(f'Importance Heat-map\\nClass: {class_label}, '\n",
        "            f'Prob: {probability:.4f}, R^2: {r2:.4f}',\n",
        "            fontsize=14\n",
        "        )\n",
        "        ax.axis('off')\n",
        "\n",
        "        # colour-bar\n",
        "        cax = plt.subplot(gs[1, :])\n",
        "        plt.colorbar(heat_img, cax=cax, orientation='horizontal')\n",
        "        cax.set_xlabel('Feature Importance (Red: Negative  Green: Positive)',\n",
        "            fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        return fig\n",
        "\n",
        "    def create_overlay_visualization(self, image, segments, seg_imp,\n",
        "                                     class_label, probability,\n",
        "                                     threshold=0.0, save_path=None):\n",
        "\n",
        "        img     = np.array(image).astype(float) / 255.0\n",
        "        if np.max(np.abs(seg_imp)) > 0:\n",
        "            seg_imp = seg_imp / np.max(np.abs(seg_imp))\n",
        "\n",
        "        pos = seg_imp > threshold\n",
        "        neg = seg_imp < -threshold\n",
        "        alpha = 0.5\n",
        "        overlay = img.copy()\n",
        "        if np.any(pos):\n",
        "          # green overlay means positively influential space\n",
        "            green = np.zeros_like(img); green[..., 1] = np.abs(seg_imp) * pos\n",
        "            overlay = overlay * (1 - alpha * pos[..., None]) + green * alpha\n",
        "        if np.any(neg):\n",
        "          # red overlay means negatively influential space\n",
        "            red = np.zeros_like(img); red[..., 0] = np.abs(seg_imp) * neg\n",
        "            overlay = overlay * (1 - alpha * neg[..., None]) + red * alpha\n",
        "\n",
        "        overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        ax.imshow(overlay)\n",
        "        ax.set_title(f'LIME Overlay {class_label} ({probability:.4f})',\n",
        "                     fontsize=14)\n",
        "        ax.axis('off')\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def find_most_influential_regions(self, segments, feat_imp, top_n=5):\n",
        "        abs_imp = np.abs(feat_imp)\n",
        "        top_idx = np.argsort(abs_imp)[::-1][:top_n]\n",
        "\n",
        "        regions = []\n",
        "        for rank, idx in enumerate(top_idx, 1):\n",
        "            regions.append({\n",
        "                'rank'       : rank,\n",
        "                'segment_id' : idx + 1,\n",
        "                'importance' : feat_imp[idx],\n",
        "                'influence'  : 'Positive' if feat_imp[idx] > 0 else 'Negative'\n",
        "            })\n",
        "        return regions\n",
        "\n",
        "    def visualize_top_regions(self, image, segments, feat_imp,\n",
        "                              top_n=5, save_path=None):\n",
        "        img_array = np.array(image)\n",
        "        abs_imp   = np.abs(feat_imp)\n",
        "        top_idx   = np.argsort(abs_imp)[::-1][:top_n]\n",
        "\n",
        "        fig, ax = plt.subplots(1, top_n + 1, figsize=(20, 5))\n",
        "\n",
        "        ax[0].imshow(mark_boundaries(img_array, segments))\n",
        "        ax[0].set_title('All Superpixels', fontsize=12)\n",
        "        ax[0].axis('off')\n",
        "\n",
        "        alpha = 0.5\n",
        "        for i, idx in enumerate(top_idx):\n",
        "            seg_id     = idx + 1\n",
        "            importance = feat_imp[idx]\n",
        "            mask       = segments == seg_id\n",
        "\n",
        "            highlighted = img_array.copy().astype(float)\n",
        "            overlay = np.zeros_like(highlighted)\n",
        "            overlay[mask] = [0, 255, 0] if importance > 0 else [255, 0, 0]\n",
        "            highlighted = highlighted * (1 - alpha) + overlay * alpha\n",
        "            highlighted = highlighted.astype(np.uint8)\n",
        "            highlighted = mark_boundaries(highlighted, mask.astype(int),\n",
        "                                          color=(1, 1, 1))\n",
        "            sign = '+' if importance > 0 else '−'\n",
        "            ax[i+1].imshow(highlighted)\n",
        "            ax[i+1].set_title(\n",
        "                f'Region {seg_id}\\n{sign}{abs_imp[idx]:.4f}',\n",
        "                fontsize=10\n",
        "            )\n",
        "            ax[i+1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhFKL5_qTJYF"
      },
      "outputs": [],
      "source": [
        "\n",
        "ROOT_DIR   = '/content/drive/MyDrive/1430project'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/1430project/explanations_batch_lg'\n",
        "\n",
        "class_names = ['fake', 'real']\n",
        "\n",
        "img_paths = []\n",
        "for cls in ['0', '1']:\n",
        "    cls_dir = os.path.join(ROOT_DIR, cls)\n",
        "    img_paths += [\n",
        "        os.path.join(cls_dir, f)\n",
        "        for f in os.listdir(cls_dir)\n",
        "        if f.lower().endswith(('.png'))\n",
        "    ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxk2WpGpdNYi"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet captum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM97YbtBdNoN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch, numpy as np\n",
        "from captum.attr import LayerGradCam, LayerAttribution\n",
        "from skimage.transform import resize\n",
        "from skimage.util import img_as_float32\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class GradCamHelper:\n",
        "    def __init__(self, model, device, target_layer=None,\n",
        "                 alpha: float = 0.5, cmap: str = 'jet'):\n",
        "        self.model   = model\n",
        "        self.device  = device\n",
        "        self.layer   = target_layer or model.conv_head   # last conv if none given\n",
        "        self.gradcam = LayerGradCam(self.model, self.layer)\n",
        "        self.alpha   = alpha\n",
        "        self.cmap    = cmap\n",
        "\n",
        "    def get_heatmap(self, image_tensor, class_idx):\n",
        "        attributions = self.gradcam.attribute(image_tensor, target=class_idx)\n",
        "        up = LayerAttribution.interpolate(attributions,\n",
        "                                          image_tensor.shape[-2:])\n",
        "        # Resize CAM to match original input res\n",
        "        heat = up.squeeze().cpu().detach().numpy()\n",
        "        heat = np.maximum(heat, 0)          # ReLU\n",
        "        if heat.max() > 0:\n",
        "            heat /= heat.max()\n",
        "        return heat\n",
        "    def save_overlay(self, heatmap, pil_image, out_path,\n",
        "                     alpha=None, cmap=None):\n",
        "        alpha = self.alpha if alpha is None else alpha\n",
        "        cmap  = self.cmap  if cmap  is None else cmap\n",
        "\n",
        "        img = img_as_float32(np.array(pil_image))\n",
        "\n",
        "        if heatmap.shape != img.shape[:2]: # make sure heatmap matches image size for overlay\n",
        "            heatmap = resize(\n",
        "                heatmap, img.shape[:2], order=1,\n",
        "                preserve_range=True, anti_aliasing=True\n",
        "            )\n",
        "\n",
        "        hm_rgb = plt.colormaps.get_cmap(cmap)(heatmap)[..., :3] # blend heatmap and image\n",
        "        overlay = (1 - alpha) * img + alpha * hm_rgb\n",
        "        overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.imshow(overlay)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53HY7llkTJZv",
        "outputId": "a63a9eb0-fe7d-44da-b226-999aa4086328"
      },
      "outputs": [],
      "source": [
        "\n",
        "lime = LimeExplainer(model, transform=TRANSFORM, device=DEVICE, output_dir=OUTPUT_DIR)\n",
        "grad_helper = GradCamHelper(model, DEVICE)\n",
        "# set up both LIME and gcam for same model\n",
        "\n",
        "for k, img_path in enumerate(sorted(img_paths), start=1):\n",
        "      image     = Image.open(img_path).convert('RGB')\n",
        "      probs     = lime.get_model_prediction(image)\n",
        "      pred_idx  = int(np.argmax(probs))\n",
        "      # get model prediction and index of top class\n",
        "      pred_name = class_names[pred_idx]\n",
        "      pred_prob = probs[pred_idx]\n",
        "      img_tensor = TRANSFORM(image).unsqueeze(0).to(DEVICE)\n",
        "      heat = grad_helper.get_heatmap(img_tensor, pred_idx)\n",
        "      base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "      # gen and save gcam overlay\n",
        "      grad_path = os.path.join(OUTPUT_DIR,\n",
        "                                f\"{base}_{pred_name}_gradcam.png\")\n",
        "      grad_helper.save_overlay(heat, image, grad_path, alpha=0.5)\n",
        "      segments, seg_img, feat_imp, r2, pred_cls, _ = lime.explain(image, pred_idx) # run lime to get feature importance\n",
        "      out_prefix = f\"{base}_{pred_name}\"\n",
        "      lime.visualize_explanation(\n",
        "          image, segments, seg_imp, pred_name, pred_prob, r2,\n",
        "          save_path=os.path.join(OUTPUT_DIR, f'{out_prefix}_explanation.png')\n",
        "      )\n",
        "      lime.create_overlay_visualization(\n",
        "          image, segments, seg_imp,\n",
        "          pred_name, pred_prob, threshold=0.0,\n",
        "          save_path=os.path.join(OUTPUT_DIR, f'{out_prefix}_overlay.png')\n",
        "      )\n",
        "\n",
        "      lime.visualize_top_regions(\n",
        "          image, segments, feat_imp, top_n=5,\n",
        "          save_path=os.path.join(OUTPUT_DIR, f'{out_prefix}_top_regions.png')\n",
        "      )\n",
        "      np.savez( # saev for later analysis if want\n",
        "          os.path.join(OUTPUT_DIR, f'{out_prefix}_data.npz'),\n",
        "          segments=segments,\n",
        "          segments_importance=seg_imp,\n",
        "          feature_importance=feat_imp,\n",
        "          r2=r2,\n",
        "          prediction=probs,\n",
        "          gradcam=heat\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDU6Sba3dNvl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
